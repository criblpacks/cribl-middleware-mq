output: default
groups:
  x3gbD4:
    description: Extract and clean critical fields [Can be replaced with Grok function]
    name: Extract critical fields
    index: 2
    disabled: false
asyncFuncTimeout: 1000
functions:
  - id: comment
    filter: "true"
    disabled: null
    conf:
      comment: >-
        Parse RabbitMQ logs for easier searching. In addition to extracting
        critical key/value pairs, remove any non-value add characters and/or
        fields, to reduce log size before ingest.


        Author: Carley Rosato

        Date:   2021-07-01

        Notes:  Come find me on Slack - https://cribl.io/community - cribl-community.slack.com
  - id: comment
    filter: "true"
    disabled: null
    conf:
      comment: Extract critical fields from _raw and reformat
  - id: regex_extract
    filter: "true"
    disabled: false
    conf:
      source: _raw
      iterations: 100
      overwrite: false
      regex: /\[(?<level>.*)\] \<(?<rabbitmq_pid>.+?\.0)\>
        (?<message>(?<=\.0\>\s).*)/gms
      regexList: []
    description: Extract Log Level, Process ID and  Message
    groupId: x3gbD4
  - id: eval
    filter: "true"
    disabled: false
    conf:
      add:
        - name: message
          value: message.trimLeft().replace(/\n\s+/g,'\n').split(/\n/).join(', ')
    description: Remove blank spaces from beginning of Message value
    groupId: x3gbD4
  - id: comment
    filter: "true"
    disabled: null
    conf:
      comment: "[OPTIONAL] Reduce the number of non-value add logs retained"
  - id: sampling
    filter: level=='debug'
    disabled: true
    conf:
      rules:
        - filter: level=='debug'
          rate: 100
    description: "[CUSTOMIZE] Retain 1 out of every 100 debug level logs"
  - id: dynamic_sampling
    filter: level=='debug'
    disabled: true
    conf:
      mode: log
      keyExpr: "`${level}`"
      samplePeriod: 30
      minEvents: 30
      maxSampleRate: 100
    description: "[CUSTOMIZE] Alternate to Static Sampling function above. Allows for
      adjusting sampling rates based on incoming data volume "
  - id: suppress
    filter: level=='info'
    disabled: true
    conf:
      allow: 1
      suppressPeriodSec: 30
      dropEventsMode: true
      maxCacheSize: 50000
      cacheIdleTimeoutPeriods: 2
      numEventsIdleTimeoutTrigger: 10000
      keyExpr: "`${level}-${rabbitmq_pid}`"
    description: "[CUSTOMIZE] Allow 1 event every 30 seconds for info level logs carrying
      out actions under the same PID"
  - id: comment
    filter: "true"
    disabled: null
    conf:
      comment: Convert to JSON and Serialize
  - id: serialize
    filter: "true"
    disabled: false
    conf:
      type: json
      dstField: _raw
      fields:
        - "!_*"
        - "!cribl_breaker"
        - "*"
      cleanFields: false
    description: Convert to JSON and push cleaned fields into _raw
  - id: eval
    filter: "true"
    disabled: false
    conf:
      add:
        - name: _raw
          value: JSON.parse(_raw)
      keep:
        - _*
      remove:
        - "*"
        - cribl*
    description: JSON parse _raw to an object
  - id: eval
    filter: "true"
    disabled: true
    conf:
      keep: []
      remove: []
    description: "[OPTIONAL] Remove non-value add fields from the events"
